# 빅데이터분석기사 2021

## 1 빅데이터 분석 기획

### CHAPTER01 빅데이터의 이해

#### 1 빅데이터 개요 및 활용

##### 1) 빅데이터 특징

###### (1) 빅데이터(Big Data) 개념

- 빅 데이터는 막대한 양(수십 테라바이트 이상)의 정형 및 비정형 데이터이다.
- 데이터로 부터 가치를 추출하고 결과를 분석하는 기술의 의미로도 통용된다.
- 데이터에서부터 가치를 추출하는 것은 통찰, 지혜를 얻는 과정으로 Ackoff, R.L. 이 도식화한 DIKW 피라미드로 표현할 수 있다.

DIKW 피라미드

1. 데이터(Data)
   - 객관적 사실로서 다른 데이터와의 상관관계가 없는 가공하기 전의 순수한 수치나 기호
   - 예) 수제비 책을 A사이트에서 30,000원, B사이트에서 35,000원에 판매
2. 정보(Information)
   - 가공, 처리하여 데이터 간의 연관 관계와 함께 의미가 도출된 데이터
   - 예) 수제비 책은 A 사이트에서 더 싸게 판매
3. 지식(Knowledge)
   - 획득한 다양한 정보를 구조화하여 유의미한 정보로 분류하고 일반화시킨 결과물
   - 정보에 기반해 찾아진 규칙
   - 예) A 사이트가 싸게 팔기 때문에 수제비 책을 구입할 계획
4. 지혜(Wisdom)
   - 근본 원리에 대한 깊은 이해를 바탕으로 도출되는 창의적 아이디어
   - 상황이나 맥락에 맞게 규칙을 적용하는 요소
   - 예) A 사이트의 다른 상품들도 B 사이트보다 저렴할 것으로 판단

데이터 양을 측정하는 바이트 크기

- 킬로(KB, 10<sup>3</sup> Bytes) > 메가(MB, 10<sup>6</sup> Bytes) > 기가(GB, 10<sup>9</sup> Bytes) > 테라(TB, 10<sup>12</sup> Bytes) > 페타(PB, 10<sup>15</sup> Bytes) > 엑사(EB, 10<sup>18</sup> Bytes) > 제타(ZB, 10<sup>21</sup> Bytes) > 요타(YB, 10<sup>24</sup> Bytes)

###### (2) 빅데이터 특징

전통적으로 3V(Volume, Variety, Velocity)의 특징이 있지만, 최근엔 5V(Veracity, Value 추가), 7V(Validity, Volatility 추가)로 확장되고 있다.

빅데이터의 특성

1. 규모(Volume)
   - 빅데이터 분석 규모에 관련된 특징
   - ICT 기술 발전으로 과거의 텍스트 데이터부터 SNS로부터 수집되는 사진, 동영상 등의 다양한 멀티미디어 데이터까지 디지털 정보량의 기하급수적 증가
2. 다양성(Variety)
   - 빅데이터 자원 유형에 관련된 특징
   - 정형 데이터뿐만 아니라 비정형, 반정형 데이터를 포함
3. 속도(Velocity)
   - 빅데이터 수집.분석.활용 속도에 관련된 특징
   - 사물 정보(센서, 모니터링), 스트리밍 정보 등 실시간성 정보의 생성 속도 증가에 따라 처리 속도 가속화 요구
   - 가치 있는 정보 활용을 위해 데이터 처리 및 분석 속도의 중요성 증가
4. 신뢰성(Veracity)
   - 빅데이터 수집 대상 데이터가 가지는 신뢰에 관련된 특징
   - 방대한 데이터에서 노이즈 및 오류 제거를 통해 활용 데이터에 대한 품질과 신뢰성 제고 요구
5. 가치(Value)
   - 빅데이터 수집 데이터를 통해 얻을 수 있는 가치
   - 비즈니스나 연구에 활용되어 유용한 가치를 끌어낼 수 있는가에 대한 문제
   - 빅데이터의 가치는 데이터의 정확성 및 시간성과 관련됨
6. 정확성(Validity)
   - 빅데이터의 수집 대상 데이터가 가지는 유효성과 정확성
   - 데이터의 규모가 아무리 크더라도 질 높은 데이터를 활용한 정확한 분석 수행이 없다면 의미가 없음
   - 데이터가 타당한지 정확한지에 대한 여부는 의사결정의 중요한 요소
7. 휘발성(Volatility)
   - 빅데이터의 수집 대상 데이터가 의미가 있는 기간
   - 데이터가 얼마나 오래 저장될 수 있고, 타당하여 오랫동안 쓰일 수 있을지에 관한 사항
   - 빅데이터는 장기적인 관점에서 유용한 가치를 창출해야 함

###### (3) 빅데이터의 유형

데이터 구조적 관점에서의 빅데이터 유형

1. 정형
   - 정형화된 스키마 구조, DBMS에 내용이 저장될 수 있는 구조
   - 고정된 필드(속성)에 저장된 데이터
   - 예) 관계형 데이터베이스(Oracle, MS-SQL 등)
2. 반정형
   - 데이터 내부의 데이터 구조에 대한 메타 정보가 포함된 구조
   - 고정된 필드에 저장되어 있지는 않지만, 메타데이터나 데이터 스키마 정보를 포함하는 데이터
   - 예) XML, HTML, JSON 등
3. 비정형
   - 수집 데이터 각각이 데이터 객체로 구분
   - 고정 필드 및 메타데이터(스키마 포함)가 정의되지 않음
   - Crawler, API, RSS 등의 수집 기술을 활용
   - 예) 텍스트 문서, 이진 파일, 이미지, 동영상 등

###### (4) 데이터 지식경영

데이터 기반 지식경영의 핵심 이슈는 암묵지와 형식지의 상호작용에 있다.

1. 암묵지
   - 학습과 경험을 통해 개인에게 체화되어 있지만 겉으로 드러나지 않는 지식
   - 사회적으로 중요하지만 다른 사람에게 공유되기 어려움
   - 예) 수영, 태권도
   - 상호작용 : 공통화, 내면화
2. 형식지
   - 문서나 매뉴얼처럼 형상화된 지식
   - 전달과 공유가 용이
   - 예) 수험서, 소프트웨어 설치 매뉴얼
   - 상호작용 : 표출화, 연결화

데이터 지식경영 상호작용

1. 내면화 : 행동과 실천교육 등을 통해 형식지가 개인의 암묵지로 체화되는 단계
2. 공통화 : 다른 사람과의 대화 등 상호작용을 통해 개인이 암묵지를 습득하는 단계
3. 표출화 : 형식지 요소 중의 하나이며 개인에게 내재된 경험을 객관적인 데이터인 문서나 매체로 저장하거나 가공, 분석하는 과정
4. 연결화 : 형식지가 상호결합하면서 새로운 형식지를 창출하는 과정


##### 2) 빅데이터의 가치

###### (1) 빅데이터의 가치

- 경제적 자산

  - 새로운 기회를 창줄하고, 위험을 해결하여 사회 및 경제 발전의 엔진 역할을 수행

- 불확실성 제거

  - 사회현상, 현실 세계의 데이터를 기반으로 한 패턴 분석과 미래 전망
  - 여러 가지 가능성에 대한 시나리오 시뮬레이션

- 리스크 감소

  - 환경, 소설, 모니터링 정보의 패턴 분석을 통해 위험 징후 및 이상 신호 포착
  - 이슈를 사전에 인지 및 분석하고 빠른 의사 결정과 실시간 대응

- 스마트한 경쟁력

  - 대규모 데이터 분석을 통한 상황 인지, 인공지능 서비스 기능
  - 개인화, 지능화 서비스 제공 확대
  - 트렌드 변화 분석을 통한 제품 경쟁력 확보

- 타 분야 융합

  - 타 분야와의 융합을 통한 새로운 가치 창출

  - 방대한 데이터 활용을 통한 새로운 융합시장 창출



###### (2) 빅데이터 가치 산정이 어려운 이유

- 데이터 활용 방식의 다양화
  - 데이터의 재사용, 데이터의 재조합, 다목적용 데이터 개발 등이 일반화되면서 특정 데이터를 언제/어디서/누가 활용할지 알 수 없어서 가치 산정이 어려움
  - 데이터의 창의적 조합으로 인해 기존에 풀 수 없는 문제를 해결하는데 도움을 주기 때문에 가치 산정이 어려움
  - 예) 구글이 검색 결과를 낼 때마다 구글은 클라우드에 저장된 웹 사이트 정보를 매번 사용
- 새로운 가치 창출
  - 빅데이터 시대에 데이터가 기존에 없던 가치를 창출하여 가치 산정이 어려움
  - 예) 고객의 성향을 분석하여 고객 맞춤 서비스 제공
- 분석기술의 급속한 발전
  - 비용 문제로 분석할 수 없었던 것을 저렴한 비용으로 분석하면서 활용도가 증가하여 가치 산정이 어려움
  - 예) 텍스트 마이닝을 통한 SNS 분석



###### (3) 빅데이터 영향

- 기업 : 혁신 수당 제공, 경쟁력 강화, 생산성 향상
- 정부 : 환경 탐색, 상황 분석, 미래 대응 가능
  - 예) 사회관계망 분석, 시스템 다이내믹스와 같은 분석 방식을 통해 미래 의제 도출
  - 사회관계망 분석(**SNA**; Social Network Analysis) : 그룹에 속한 사람들 간의 네트워크 특성과 구조를 분석하고 시각화하는 분석기법
  - 시스템 다이내믹스(System Dynamics) : 사업이나 사회 시스템 등과 같은 복잡한 피드백 시스템을 연구하고 관리하는 방법이다.
- 개인 : 목적에 따른 활용
  - 예) 빅데이터 서비스를 저렴한 비용으로 활용. 적시에 필요한 정보를 획득



###### (4) 빅데이터 위기 요인 및 통제 방안

빅데이터 위기요인

- 사생활 침해
- 책임 원칙 훼손
- 데이터 오용

빅데이터 위기 요인에 대한 통제 방안

- 알고리즘에 대한 접근 허용
- 책임의 강조
- 결과 기반의 책임 적용



##### 3) 빅데이터 산업의 이해

###### (1) 빅데이터 산업 개요

- 스마트폰, SNS, IoT 확산 등에 따라 데이터 활용이 증가하여 빅데이터는 신성장동력으로 급부상 중
- 클라우드 컴퓨팅 기술의 발전으로 데이터 처리 비용이 급격히 감소하여 빅데이터가 발전 중
- 주요국 및 글로벌 기업은 빅데이터 '산업' 육성 및 '활용'에 주력 중
- 우리나라는 데이터 생산량이 많은 산업(통신.제조업 등)이 발달해 잠재력이 크지만, 불확실성에 따른 투자 리스크 등으로 '활용'은 저조하다

###### (2) 산업별 빅데이터 활용

- 의료.건강
- 과학기술
- 정보보안
- 제조.공정
- 소비.거래
- 교통.물류



##### 4) 빅데이터 조직 인력

###### (1) 빅데이터 조직 설계

빅데이터 업무 프로세스

1. 빅데이터 도입 단계
   - 빅데이터 서비스를 제공하기 위해서는 빅데이터 시스템 구축을 위한 빅데이터 도입 기획, 기술 검토, 도입 조직 구성, 예산 확보 등을 수행
1. 빅데이터 구축 단계
   - 빅데이터 플랫폼을 구축하기 위해서는 요구사항 분석, 설계, 구현, 테스트 단계를 수행
1. 빅데이터 운영 단계
   - 빅데이터 시스템의 도입 및 구축이 끝나면, 이를 인수하여 운영 계획을 수립
   - 빅데이터 플랫폼 운영, 데이터 및 빅데이터 분석 모델 운영, 빅데이터 운영 조직, 운영 예산 고려



###### (2) 조직 역량

데이터 사이언티스트의 요구역량

- 소프트 스킬(Soft Skill)
  - 분석의 통찰력 : 논리적 비판 능력, 창의적 사고력, 호기심
  - 여러 분야의 협력 능력 : 커뮤니케이션 능력
  - 설득력 있는 전달력 : 스토리텔링 능력, 비쥬얼라이제이션
- 하드 스킬(Hard Skill)
  - 빅데이터 관련 이론적 지식 : 빅데이터 관련 기법 및 다양한 방법론 습득
  - 분석기술의 숙련도 : 목적에 맞는 최적 분석 설계, 노하우 축적

가트너(Gartner)는 데이터 사이언티스트가 갖추어야 할 역량으로 분석 모델링, 데이터 관리, 소프트 스킬, 비즈니스 분석을 제시했다.



역량 모델 개발 절차

1. 조직의 미션/성과 목표/CSF 검토 : 조직이 구성되면 조직의 미션, 조직의 성과 목표를 달성하기 위하여 CSF를 검토

> **CSF**(Critical Success Factor; 핵심 성공 요인) : 목표 성취를 위해 필요한 요소를 뜻하는 용어로 기업 경쟁력 향상을 위한 핵심 내부 역량이며, 목표 달성을 위해 반드시 수행해야 하는 필수 요소이다.

2. 조직 구성원의 행동 특성 도출 : 성과 목표 달성을 잘하는 우수 성과자의 행동 특성을 파악하여 도출
1. 조직 구성원의 역량 도출 : 도출된 행동 특성을 기반으로 지식, 스킬, 태도 등을 도출하여 직무별 역량 모델을 생성
1. 조직 구성의 역량 모델 확정 : 직무별 역량 모델을 업무 전문가, 인사 담당자가 검토하고 협의하여 확정



역량 교육 체계 설계 절차

1. 요구사항 분석
1. 직무별 역량 모델 검토
1. 역량 차이 분석
1. 직무 역량 매트릭스 작성
1. 직무별 역량 교육 체계 설계



###### (3) 조직성과 평가

> **KPI**(Key Performance Indicator; 핵심 성과 지표) : 사업, 부서, 혹은 개인 차원의 목표가 달성되었는지 그 실적을 추적하기 위한 정량화된 측정 지표이다. (예를 들어 1인당 1년에 30개의 카드 발급 추진)

조직성과 평가 절차

1. 목표 설정
2. 모니터링
3. 목표 조정
4. 평가 실시
5. 결과의 피드백



균형 성과표(**BSC**; Balanced Score Card) 4가지 관점

- 재무 : 기업의 주요 이해 관계자들에게 재무적인 지표를 통해 조직의 성과를 보여주기 위한 관점
- 고객 : 고객 관계 관리를 위한 관점. 기업에 수익을 가져다줄 수 있는 고객을 파악해 내고, 이들을 위한 고객 지향적 프로세스를 만들어나가는 것이 고객 관계 관리의 핵심 성공 요인
- 내부 프로세스 : 성과를 극대화하기 위하여 기업의 핵심 프로세스 및 핵심 역량을 규명하는 과정에 관련한 관점
- 학습.성장 : BSC의 4가지 관점 중에서 가장 미래 지향적인 관점. 현재에는 그 가치가 보이지 않지만, 회사의 장기적인 잠재력에 대한 투자가 기업 성정에 얼마나 영향을 미칠 수 있을지를 파악



BSC를 통한 KPI 도출 예시

|     관점      | KPI 지표                      | KPI 목표               | 계산방식(단위)               | 평가기준                             |
| :-----------: | ----------------------------- | ---------------------- | ---------------------------- | ------------------------------------ |
|     재무      | 빅데이터 분석비용 절감        | 10% 절감               | 전년 대비 분석비용 절감률(%) | 1등급 :10% 이상<br>2등급 : 5% 이상   |
|     고객      | 빅데이터 서비스 만족도        | 별 5점 만점에 4점 이상 | 고객 평가 점수(점)           | 1등급 : 평균 4점<br>2등급 : 평균 3점 |
| 내부 프로세스 | 서비스 가동률                 | 99% 가용성 확보        | 서비스 가동률(%)             | 1등급 : 99%<br>2등급 : 98%           |
|   학습.성장   | 1인당 빅데이터 분석 교육 시간 | 1인당 24시간 이상      | 교육 시간(시간)              | 1등급 : 24시간<br>2등급 : 16시간     |



#### 2 빅데이터 기술 및 제도

##### 1) 빅데이터 플랫폼

###### (1) 빅데이터 플랫폼(Bigdata Platform)의 개념

- 빅데이터에서 가치를 추출하기 위해 일련의 과정(수집 > 저장 > 처리 > 분석 > 시각화)을 규격화한 기술이다.

###### (2) 빅데이터 플랫폼 구성요소

|  구성요소   | 주요기능                                                     |
| :---------: | :----------------------------------------------------------- |
| 데이터 수집 | 원천 데이터의 정형/반정형/비정형 데이터 수집<br>ETL, 크롤러, EAI(Enterprise Architecture Integration) 등 |
| 데이터 저장 | 정형 데이터, 반정형 데이터, 비정형 데이터 저장<br>RDBMS, NoSQL 등 |
| 데이터 분석 | 텍스트 분석, 머신러닝, 통계, 데이터 마이닝<br>SNS 분석, 예측 분석 등 |
| 데이터 활용 | 데이터 가시화 및 BI, Open API 연계<br>히스토그램, 인포그래픽 등 |

용어설명

> **크롤러**(Crawler) : URL에 존재하는 HTML 문서에 접근하여 해당 내용을 추출하고, 문서에 포함된 하이퍼링크를 통해 재귀적으로 다른 문서에 접근하여 콘텐츠 수집을 반복하는 기술
>
> **EAI**(Enterprise Architecture Integration) : 기업에서 운영하는 서로 다른 기종의 애플리케이션 및 시스템을 통합하는 솔루션
>
> **NoSQL**(Not Only SQL) : NoSQL은 전통적인 RDBMS와 다른 DBMS를 지칭하기 위한 용어로서 데이터 저장에 고정된 테이블 스키마가 필요하지 않고 조인(Join) 연산을 사용할 수 없으며, 수평적 확장이 가능한 DBMS 이다.
>
> **BI**(Business Intelligence) : 데이터를 통합/분석하여 기업 활동에 연관된 의사결정을 돕는 프로세스이다.
>
> **히스토그램**(Histogram) : 자료 분포의 형태를 직사각형 형태로 시각화하여 보여주는 차트, 수평축에는 각 계급을 나타내고, 수직축에는 도수 또는 상대도수를 나타낸다.
>
> **인포그래픽**(Infographics) : 인포그래픽은 Information + Graphic의 줄임말. 중요 정보를 하나의 그래픽으로 표현해서 보는 사람들이 쉽게 정보를 이해할 수 있도록 만드는 시각화 방법



###### (3) 빅데이터 플랫폼 데이터 형식

* HTML
  * HyperText Markup Language 약자
  * 웹 페이지를 만들 때 사용되는 문서 형식
  * 텍스트, 태그, 스크립트로 구성
* XML
  * eXtensible Markup Language의 약자
  * SGML 문서 형식을 가진, 다른 특수한 목적을 갖는 마크업 언어를 만드는데 사용하는 다목적 마크업 언어
  * 데이터 표현을 위해 태그 사용
  * 엘리먼트, 속성, 처리 명령, 엔티티, 주석, CDATE 섹션으로 구성
* CSV
  * Comma Separated Values의 약자
  * 몇 가지 필드를 쉼표(,)로 구분한 텍스트 데이터 및 텍스트 파일
* JSON
  * JavaScript Object Notation의 약자
  * <키-값>으로 이루어진 데이터 오브젝트를 전달하기 위해 텍스트를 사용하는 개방형 포맷



###### (4) 빅데이터 플랫폼 구축 소프트웨어

| 소프트웨어  |       핵심        | 목적                                                         |
| :---------: | --------------- | ------------------------------------------------------------ |
|      R      |   빅데이터 분석   | 통계 프로그래밍 언어인 S 언어를 기반으로 만들어진 오픈소스 프로그래밍 언어<br>다양한 그래프 패키지들을 통하여 강력한 시각화 기능 제공 |
| 우지(Oozie) |  워크플로우 관리  | 하둡 작업을 관리하는 워크플로우 및 코디네이터 시스템(스케줄링/모니터링)<br>맵리듀스나 피그와 같은 특화된 액션들로 구성된 워크플로우 제어 |
| 플럼(Flume) |    데이터 수집    | 이벤트와 에이전트(Agent)를 활용하여 많은 양의 로그 데이터를 효율적으로 수집, 집계, 이동 |
|    HBase    | 분산 데이터베이스 | 컬럼 기반 저장소로 HDFS와 인터페이스 제공                    |
| 스쿱(Sqoop) | 정형 데이터 수집  | 'SQL to Hadoop'의 약자<br>커넥터(Connector)를 사용하여 RDBMS 에서 하둡 파일 시스템(HDFS)으로 데이터를 수집하거나, HDFS에서 RDBMS로 데이터를 보내는 기능 수행 |



분산 컴퓨팅 환경 소프트웨어 구성요소

- 맵리듀스(Map Reduce)
  - Key - Value 형태의 데이터 처리
  - 맵(Map) > 셔플(Shuffle) > 리듀스(Reduce) 순서대로 데이터 처리
    - 맵 : Key - Value 형태로 데이터를 취합
    - 셔플 : 데이터를 통합하여 처리
    - 리듀스 : 맵 처리된 데이터를 정리
- 얀 (YARN)
  - 하둡의 맵리듀스 처리 부분을 새롭게 만든 자원 관리 플랫폼
  - 리소스 매니저(Master)와 노드 매니저(Slave)로 구성
    - 리소스 매니저 : 스케줄러 역할을 수행하고 클러스터 이용률 최적화를 수행
    - 노드 매니저 : 노드 내의 자원을 관리하고 리소스 매니저에게 전달 수행 및 컨테이너를 관리
    - 애플리케이션 마스터 : 리소스 매니저와 자원의 교섭을 책임지고, 컨테이너를 실행
    - 컨테이너 : 프로그램 구동을 위한 격리 환경을 지원하는 가상화 자원
- 아파치 스파크(Apache Spark)
  - 하둡 기반 대규모 데이터 분산처리시스템
  - 스트리밍 데이터, 온라인 머신러닝 등 실시간 데이터 처리
  - 스칼라, 자바, 파이썬, R 등에 사용
- 하둡 분산 파일 시스템(HDFS)
  - Hadoop Distributed File Sytem 의 약자
  - 대용량 파일을 분산된 서버에 저장하고, 그 저장된 데이터를 빠르게 처리할 수 있게 하는 하둡 분산 파일 시스템
  - 네임 노드(Master)와 데이터 노드(Slave)로 구성
    - 네임 노드 : 파일 이름, 권한 등의 속성 기록
    - 데이터 노드 : 일정한 크기로 나눈 블록 형태로 저장
- 아파치 하둡 (Apache Hadoop)
  - 분산 파일 시스템(HDFS)과 맵리듀스를 중심으로 다양한 프로그램으로 구성된 하둡 에코시스템을 가짐
  - 클라우드 플랫폼 위에서 클러스터를 구성해 데이터 분석
  - 예) Spark, Hive, YARN, Cassandra, Pig 등



###### (5) 하둡 에코시스템(Hadoop Ecosystem)

하둡 프레임워크를 이루고 있는 다양한 서브 프로젝트들의 모임

하둡 에코시스템 수집, 저장, 처리 기술

- 비정형 데이터 수집
  - 척와(Chukwa) : 분산된 각 서버에서 에이전트를 실행하고, 컬렉터(Collector)가 에이전트로부터 데이터를 받아 HDFS에 저장
  - 플럼(Flume) : 많은 양의 로그 데이터를 효율적으로 수집, 집계, 이동하기 위해 이벤트와 에이전트(Agent)를 활용하는 기술
  - 스크라이브(Scribe)
    - 다수의 서버로부터 실시간으로 스트리밍되는 로그 데이터를 수집하여 분산 시스템에 데이터를 저장하는 대용량 실시간 로그 수집 기술
    - 최종 데이터는 HDFS 외에 다양한 저장소를 활용 가능
    - HDFS에 저장하기 위해서는 JNI(Java Native Interface)를 이용
- 정형 데이터 수집
  - 스쿱(Sqoop)
    - 대용량 데이터 전송 솔루션
    - 커넥터를 사용하여 RDBMS에서 HDFS로 데이터를 수집하거나, HDFS에서 RDBMS로 데이터를 보내는 기능 수행
    - Oracle, MS-SQL, DB2와 같은 상용 RDBMS와 MySQL과 같은 오픈소스 RDBMS 지원
  - 히호(Hiho)
    - 스쿱과 같은 대용량 데이터 전송 솔루션이며, 현재 깃허브에서 공개되어 있음
    - 하둡에서 데이터를 가져오기 위한 SQL을 지정할 수 있으며, JDBC 인터페이스를 지원, 현재는 Oracle, MySQL의 데이터만 전송 지원
- 분산 데이터 저장
  - HDFS
    - 대용량 파일을 분산된 서버에 저장하고, 그 저장된 데이터를 빠르게 처리할 수 있게 하는 하둡 분산 파일 시스템
    - 범용 하드웨어 기반, 클러스터에서 실행되고 데이터 접근 패턴을 스트리밍 방식으로 지원
    - 다중 복제, 대량 파일 저장, 온라인 변경, 범용서버 기반, 자동복구 특징이 있음
- 분산 데이터 처리
  - 맵리듀스(Map Reduce)
    - 대용량 데이터 세트를 분산 병렬 컴퓨팅에서 처리하거나 생성하기 위한 목적으로 만들어진 소프트웨어 프레임워크
    - 모든 데이터를 Key - Value 쌍으로 구성, 데이터를 분류
- 분산 데이터 베이스
  - HBase
    - 컬럼 기반 저장소로 HDFS와 인터페이스 제공
    - 실시간 랜덤 조회 및 업데이트를 할 수 있으며, 각각의 프로세스는 개인의 데이터를 비동기적으로 업데이트할 수 있음



하둡 에코시스템의 데이터 가공 및 분석, 관리를 위한 주요 기술

- 데이터 가공
  - 피그(Pig)
    - 대용량 데이터 집합을 분석하기 위한 플랫폼으로 하둡을 이용하여 맵리듀스를 사용하기 위한 높은 수준의 스크립트 언어인 **피그 라틴**이라는 자체 언어를 제공
    - 맵리듀스 API를 매우 단순화시키고, SQL과 유사한 형태로 설계됨
    - SQL과 유사하기만 할 뿐, 기존 SQL 지식을 활용하는 것이 어려움
  - 하이브(Hive)
    - 하둡 기반의 DW(Data Warehouse) 솔루션
    - SQL과 매우 유사한 HiveQL이라는 쿼리를 제공
    - HiveQL은 내부적으로 맵리듀스로 변환되어 실행됨
- 데이터 마이닝
  - 머하웃(Mahout)
    - 하둡 기반으로 데이터 마이닝 알고리즘을 구현한 오픈소스
    - 분류, 클러스터링, 추천 및 협업 필터링, 패턴 마이닝, 회귀 분석, 진화 알고리즘 등 주요 알고리즘 지원
- 실시간 SQL 질의
  - 임팔라(Impala)
    - 하둡 기반의 실시간 SQL 질의 시스템
    - 데이터 조회를 위한 인터페이스로 HiveQL을 사용
    - 수초 내에 SQL 질의 결과를 확인할 수 있으며, HBase와 연동이 가능
- 워크플로우 관리
  - 우지(Oozie)
    - 하둡 작업을 관리하는 워크플로우 및 코디네이터 시스템
    - 자바 서블릿 컨테이너에서 실행되는 자바 웹 애플리케이션 서버
    - 맵리듀스나 피그와 같은 특화된 액션들로 구성된 워크플로우 제어
- 분산 코디네이션
  - 주키퍼(Zookeeper)
    - 분산 환경에서 서버들 간에 상호 조정이 필요한 다양한 서비스를 제공
    - 하나의 서버에만 서비스가 집중되지 않도록 서비스를 알맞게 분산하여 동시에 처리
    - 하나의 서버에서 처리한 결과를 다른 서버들과도 동기화하여 데이터의 안정성을 보장

> > 피그 라틴(Pig Latin) : 데이터의 흐름을 표현하기 위해 사용하는 언어이다.


##### 2) 빅데이터와 인공지능 (중요도 하)

###### (1) 인공지능의 개념

인간의 지적능력을 인공적으로 구현하여 컴퓨터가 인간의 지능적인 행동과 사고를 모방할 수 있도록 하는 sw

###### (2) 빅데이터와 인공지능의 관계

- 1950년에 등장한 인공지능을 최신 트렌드로 끌고 온 것은 '빅데이터'의 존재
- 빅데이터는 비정형 데이터를 고속으로 분석할 수 있고, 이러한 점은 인공지능이 기존에 기계가 인지하지 못했던 정보들을 분석할 수 있게 한다.
- **인공지능의 암흑기**를 지나 빅데이터를 통해 자체 알고리즘을 가지고 학습하는 **딥러닝** 기술로 특정 분야에서 인간의 지능을 뛰어넘는 능력을 갖추게 되었다.

> 딥러닝(Deep Learning) : 사람의 개입이 필요한 기존의 지도 학습(Supervised Learning) 보다 더 능동적인 비지도 학습(Unsupervised Learning)이 결합되어 컴퓨터가 마치 사람처럼 스스로 학습할 수 있는 인공지능 기술

###### (3) 빅데이터와 인공지능의 전망

- 상호보완 관계로 빅데이터는 인공지능 구현 완성도를 높여주고, 빅데이터는 인공지능을 통해 문제 해결 완성도를 높인다.
- 빅데이터 기술이 주목받는 이유는 우수한 정보처리를 바탕으로 의미 있는 결과를 도출할 수 있다는 점이다.
- 빅데이터 목표가 인공지능 목표와 부합하고, 인공지능 판단을 위해서는 빅데이터와 같은 기술이 필요하므로, 빅데이터는 인공지능을 위한 기술이 될 가능성이 크다.



##### 3) 개인정보보호법.제도

###### (1) 개인정보보호의 개념

개인정보보호는 정보 주체(개인)의 **개인정보 자기 결정권**을 철저히 보장하는 활동을 의미한다.

> 개인정보 자기 결정권 : 자신에 관한 정보가 언제, 어떻게 그리고 어느 범위까지 타인에게 전달되고 이용될 수 있는지를 그 정보 주체가 스스로 결정할 수 있는 권리

###### (2) 개인정보보호의 필요성

|        필요성        | 세부 내용                                                    |
| :------------------: | ------------------------------------------------------------ |
|  유출 시 피해 심각   | 개인적 피해(정신적/경제적)와 함께 사회적 혼란 야기           |
| 정보사회 핵심 인프라 | 정보사회에서 모든 경제활동의 중심이 개인정보를 매개로 운영   |
| 개인정보 자기 통제권 | 정보 주체는 자신과 관련된 정보의 수집, 이용, 공개, 제공에 대해 본인이 통제할 수 있는 권리가 있음 |


###### (3) 빅데이터 개인정보보호 가이드라인

- 개인정보 비식별화
  - 수집 시부터 개인 식별 정보에 대한 철저한 비식별화 조치
  - 개인정보가 포함된 공개 정보 및 이용 내역 정보는 비식별화 조치를 취한 후 수집, 저장, 조합, 분석 및 제3자 제공 등 가능
- 개인정보 재식별 시 조치
  - 개인정보 재식별 시, 즉시 파기 및 비식별화 조치
  - 빅데이터 처리 과정 및 생성정보에 개인정보가 재식별될 경우, 즉시 파기하거나 추가적인 비식별화 조치 시행
- 민감정보 처리
  - 민감정보 및 통신비밀의 수집, 이용, 분석 등 처리 금지
  - 특정 개인의 사상, 신념, 정치적 견해 등 민감정보의 생성을 목적으로 정보의 수집, 이용, 저장, 조합, 분석 등 처리 금지
  - 이메일, 문자, 메시지 등 통신 내용의 수집, 이용, 저장, 조합분석 등 처리 금지
- 투명성 확보
  - 빅데이터 처리 사실, 목적 등의 공개를 통한 투명성 확보
  - 개인정보 취급방침을 통해 비식별화 조치 후 빅데이터 처리 사실, 목적, 수집 출처 및 정보 활용 거부권 행상 방법 등을 이용자에게 투명하게 공개
  - 개인정보 취급방침
    - 비식별화 조치 후 빅데이터의 처리 사실, 목적 등을 이용자에게 공개
    - <정보 활용 거부 페이지 링크>를 제공하여 이용자가 거부권을 행사할 수 있도록 조치
  - 수집 출처 고지
    - 이용자 이외의 자로부터 수집한 개인정보 처리 시 <수집 출저, 목적, 개인정보 처리 정지 요구권>을 이용자에게 고지
- 수집정보의 **보호조치**
  - 수집된 정보의 저장관리 시 기술적, 관리적 보호조치
  - 비식별화 조치가 취해진 정보를 저장관리하고 있는 정보처리시스템에 대한 기술적, 관리적 보호조치 적용

> 보호조치 : 침입 차단시스템 등 접근 통제장치 설치, 접속 기록에 대한 위.변조 방지 조치, 백신 소프트웨어 설치 운영 등 악성 프로그램에 의한 침해 방지 조치이다.



###### (4) 개인정보보호 관련 법령

|            관련 법규            | 주요 내용                                                    |
| :-----------------------------: | ------------------------------------------------------------ |
|         개인정보 보호법         | 개인정보 처리 과정상의 정보 주체와 개인정보 처리자의 권리, 의무 등 규정 |
|          정보통신망법           | '정보통신망 이용촉진 및 정보보호 등에 관한 법률'의 약칭<br>정보통신망을 통하여 수집, 처리, 보관, 이용되는 개인정보의 보호에 관한 규정 |
|           신용정보법            | '신용정보의 이용 및 보호에 관한 법률'의 약칭<br>개인 신용정보의 취급 단계별 보호조치 및 의무사항에 관한 규정 |
|           위치정보법            | '위치정보의 보호 및 이용 등에 관한 법률'의 약칭<br>개인 위치정보 수집, 이용, 제공 파기 및 정보 주체의 권리 등 규정 |
| 개인정보의 안전성 확보조치 기준 | 개인정보 처리자가 개인정보를 처리함에 있어서 분실, 도난, 유출, 변조, 훼손되지 않도록 안전성을 확보하기 위해 취해야 하는 세부적인 기준 규정<br>개인정보 처리시스템의 보호 수준을 진단, 암호화에 상응하는 조치 필요 여부를 판단할 수 있는 기준을 규정 |

> 개망신법 : 주요 3법인 개인정보보호법, 정보통신망법, 신용정보법을 현업에서 줄여서 표현
>
> 마이 데이터(My Data) : 개인이 자신의 정보를 관리.통제할 뿐만 아니라 이러한 정보를 신용이나 자산관리 등에 능동적으로 활용하는 일련의 과정



###### (5) 개인정보보호 내규

|           내규            | 주요 내용                                                    |
| :-----------------------: | ------------------------------------------------------------ |
|  정보보호 업무처리 지침   | 정보보호 조치, 개인정보 수집, 개인정보 처리 안정성 확보<br>정보보호 시스템 운영 등 각종 행정처리 절차 명사 |
|     개발 보안 가이드      | 소포트웨어 개발 시 보안 약점 제거<br>보안성을 높이는 개발 기법 가이드 마련 |
|  개인정보 암호화 매뉴얼   | 꼭 필요한 최소한의 사용자만 개인정보 접근 허용<br>개인정보 파일 암호화 저장, 사용 |
| 소포트웨어 개발 보안 구조 | 정보보안 통제 구조<br>전체적인 **정보기술 아키텍처**와의 관련성 명시 |
|    기술적, 관리적 보호    | 개인정보의 분실, 도난, 누출, 변조, 훼손 방지 방법 마련       |

> 정보기술 아키텍처(Enterprise Architecture) : 일정한 기준과 절차에 따라 업무, 응용, 데이터, 기술, 보안 등 조직 전체의 정보화 구성요소들을 통합적으로 분석한 뒤 이들 간의 관계를 구조적으로 정리한 체제 및 이를 바탕으로 정보시스템을 효율적으로 구성하기 위한 방법이다



##### 4) 개인정보 활용

###### (1) 개인정보 비식별화 개념 (법 개정 중이니 커뮤니티 통해 확인)

데이터 값 삭제, 가명처리, 총계처리, 범주화, 데이터 마스킹 등을 통해 개인정보의 일부 또는 전부를 삭제하거나 대체함으로써 다른 정보와 쉽게 결합하여도 특정 개인을 식별할 수 없도록 하는 조치를 말함



###### (2) 개인정보 비식별화 절차

1. 사전검토
   - 데이터가 개인정보에 해당하는지 검토
   - 개인정보가 아닐 경우 법적 규제 없이 자유롭게 활용
   - 개인정보일 경우 비식별 조치를 수행
2. 비식별 조치
   - 데이터 집합에서 개인을 식별할 수 있는 요소를 전부 또는 일부 삭제하거나 대체하는 등의 방법을 활용해 개인을 알아볼 수 없도록 하는 조치
3. 적정성 평가
   - 다른 정보와 쉽게 결합하여 개인을 식별할 수 있는지를 비식별 조치, 적정성 평가단을 통해 평가
4. 사후관리
   - 비식별 정보 안전조치, 재식별 가능성 모니터링 등 비식별 정보 활용 과정에서 재식별 방지를 위해 필요한 조치 수행



###### (3) 개인정보 비식별 조치 방법

| 기법                                | 세부기술                                                     | 예시                                                         |
| ----------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 가명처리<br>(Pseudonymization)      | 휴리스틱 익명화<br>암호화<br>교환방법                        | 개인 식별이 가능한 데이터에 대하여 직접 식별할 수 없는 다른 값으로 대체하는 기법<br>예) 장길산, 20세, 인천 거주 > 김식별, 20대, 인천 거주 |
| 총계처리<br>(Aggregation)           | 총계처리 기본 방식<br>부분 집계<br>라운딩<br>데이터 재배열   | 개인 정보에 대하여 통곗값을 적용하여 특정 개인을 판단할 수 없도록 하는 기법<br>예) 장길정 160cm, 김식별 150cm > 물리학과 학생 키 핪: 310cm, 평균키 : 155cm |
| 데이터 삭제<br>(Data Reduction)     | 속성값 삭제<br>속성값 부분 삭제<br>준 식별자 제거를 통한 단순 익명화 | 개인정보 식별이 가능한 특정 데이터값 삭제 처리 기법<br>예) 주민번호 801212-1234567 > 80년대 생, 남자, 개인과 관련된 날짜 정보는 연 단위로 처리 |
| 데이터 범주화<br>(Data Suppression) | 범주화 기본 방식<br>랜덤 올림 방식<br>범위 방법<br>세분 정보 제한 방법<br>제어 올림 방법 | 단인 식별 정보를 해당 그룹의 대푯값으로 변환(범주화)하거나 구간 값으로 변환(범위화)하여 고유 정보 추적 및 식별 방지 기법<br>예) 장길산, 41세 > 장 씨, 40~50세 |
| 데이터 마스킹<br>(Data Masking)     | 임의 잡음 추가 방법<br>공백과 대체 방법                      | 개인 식별 정보에 대하여 전체 또는 부분적으로 대체값(공백, '*', 노이즈 등)으로 변환<br>예) 장길산 41세, 서울 거주, 미래대학 재학 > 장OO, 41세, 서울 거주, OO대학 재학 |



###### (4) 재식별 가능성 모니터링

재식별 가능 모니터링 점검 항목

- 내부 요인의 변화
  - 비식별 조치된 정보와 연계하여 재식별 우려가 있는 추가적인 정보를 수집하였거나 제공받은 경우
  - 데이터 이용과정에서 생성되는 정보가 비식별 정보와 결합하여 새로운 정보가 생성되는 경우
  - 이용부서에서 비식별 정보에 대한 비식별 수준을 낮추어 달라고 하는 요구가 있는 경우
  - 신규 또는 추가로 구축되는 시스템이 비식별 정보에 대한 접근을 관리 통제하는 보안체계에 중대한 변화를 초래하는 경우
- 외부 환경의 변화
  - 이용 중인 데이터에 적용된 비식별 조치 방법과 유사한 방법으로 비식별 조치한 사례가 재식별되었다고 알려진 경우
  - 이용 중인 데이터에 적용된 비식별 기법과 기술을 무력화하는 새로운 기술이 등장하거나 공개된 경우
  - 이용 중인 데이터와 새롭게 연계 가능한 정보가 출현하거나, 공개된 것으로 알려진 경우


------



### CHAPTER02 데이터 분석 계획

#### 1 분석 방안 수립

##### 1) 분석 로드맵 설정

분석 로드맵의 개념과 단계는 문제 출제 어려움. 대략적으로 봐라

###### (1) 분석 로드맵 개념

단계별로 추진하고자 하는 목표를 명확히 정의하고, 선.후행 단계를 고려해 단계별 추진내용을 정렬한다.

###### (2) 분석 로드맵 단계

| 단계                       | 추진과제                                                     | 추진 목표                                                    |
| -------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 데이터 분석체계 도입       | 분석 기회 발굴 <br> 분석 과제 정의<br> 로드맵 수립           | 비즈니스 약점이 무엇인지 식별<br> 분석 과제를 정의하고 로드맵 수립 |
| 데이터 분석 유효성 검증    | 분석 알고리즘 설계<br> 아키텍처 설계<br> 분석 과제 파일럿 수행 | 분석 과제에 대한 파일럿 수행<br> 유효성, 타당성음 검증<br> 기술 실현 가능성을 검증<br> 분석 알고리즘 및 아키텍처 설계 |
| 데이터 분석 확산 및 고도화 | 변화관리<br> 시스템 구축<br> 유관 시스템 고도화              | 검증된 분석 과제를 업무 프로세스에 내재화하기 위한 변화관리 실시<br> 빅데이터 분석, 활용 시스템 구축 및 유관시스템을 고도화 |



##### 2) 분석 문제 정의

###### (1) 분석 문제의 의미

- '과제'는 처리해야 할 문제(이슈)이며, '분석'은 과제와 관련된 현상이나 원인, 해결방안에 대한 자료를 수집 및 분석하여 의사 결정에 활용하는 활동이다.
- 하향식 접근 방식과 상향식 접근 방식을 반복적으로 수행하면서 상호 보완하여 분석 과제를 발굴한다.
- 과제 발굴 이후 '분석 과제 정의서' 산출물을 작성한다.




###### (2) 하향식 접근 방식

- 하향식 접근 방식(Top Down Approach) 개념

  - 분석 과제가 정해져 있고 이에 대한 해법을 찾기 위해 체계적으로 분석하는 방법

- 하향식 접근 방식을 이용한 과제 발굴 절차

  1. 문제 탐색

     - 비즈니스 모델 기반 문제 탐색(업무, 제품, 고객, 규제와 감사, 지원 인프라 5가지 영역으로 기업 비즈니스 분석)
     - 분석 기회 발굴의 범위 확장(거시적, 경쟁사, 시장, 역량)
     - 외부 참조 모델 기반 문제 탐색(동종 사례 벤치마킹)
     - 분석 유스케이스 정의

  2. 문제 정의

     - 사용자 관점에서 비즈니스 문제를 데이터 문제로 변환하여 정의
     - 필요한 데이터 및 기법 정의

  3. 해결방안 탐색

     - 정의된 문제를 해결하기 위해 분석기법 및 역량에 따라 다양한 방안으로 탐색
     - 데이터, 시스템, 인력 등에 따라 소요되는 예산 및 활용 가능한 도구를 다양하게 고려

  4. 타당성 검토

     - 제시된 대안에 대한 타당성 평가 수행
     - 결제적 타당성(비용 대비 편익) 검토
     - 데이터 및 기술적 타당성 검토(데이터 존재 여부, 분석 시스템 환경 분석, 데이터 분석 역량 존재 여부)
     - 운영적 타당성 검토(조직의 문화, 여건 등을 감안하여 실제 운영 가능성에 대한 타당성 평가)

  5. 선택

     - 여러 대안 중 타당성에 입각하여 최적 대안의 선택하여 이를 프로젝트화하고 계획 단계의 입력 정보로 설정함

     > 비용 (Cost) : 데이터, 시스템, 인력, 유지보수와 같은 분석 비용이다.
     >
     > 편익(Benefit) : 결과 적용에 따른 실질적 비용 절감, 매출 증대 등의 결제적 가치이다.



###### (3) 상향식 접근 방식

- 상향식 접근 방식(Bottom Up Approach) 개념
  - 문제 정의 자체가 어려운 경우 데이터를 기반으로 문제를 지속적으로 개선하는 방식
  - 기존 하향식 접근법의 한계를 극복하기 위한 분석 방법론으로써 디자인 사고 접근법을 사용하여 객관적인 데이터 그 자체를 관찰하고 실제적으로 행동에 옮겨 대상을 이해하는 방식을 적용한다.
- 상향식 접근 방식 특징
  - 비지도 학습 방법
    - 데이터 자체의 결합, 연관성, 유사성 등을 중심으로 데이터의 상태 분석
    - 장바구니 분석, 군집 분석, 기술 통계, 프로파일링 등의 기술을 사용
  - 프로토타이핑 접근법 사용
    - 시행착오를 통한 문제 해결을 위해 사용
    - 가설의 생성(Hypotheses), 디자인에 대한 실험(Design Experments), 실제 환경에서의 테스트, 테스트 결과에서의 통찰(Insight) 도출 및 가설 확인의 프로세스로 실행



###### (4) 대상별 분석 기획 유형

- 최적화(Optimization)
  - 분석의 대상이 무엇인지를 인지하고 있는 경우(Known), 즉 해결해야 할 문제를 알고 있고 이미 분석의 방법도 알고 있는 경우(Known) 사용
  - 개선을 통한 최적화 형태로 분석을 수행
- 솔루션(Solution)
  - 분석의 대상은 인지(Known)하고 있으나 방법을 모르는 경우(Un-Knwon)에는 해당 분석 주체에 대한 솔루션을 찾아냄
- 통찰(Insight)
  - 분석의 대상이 명확하게 무엇인지 모르는 경우(Un-Known)에는 기존 분석 방식을 활용(Known)하여 새로운 지식인 통찰을 도출
- 발견(Discovery)
  - 분석의 대상과 방법을 모르는 경우(Un-Known)에는 발견 접근법으로 분석의 대상 자체를 새롭게 도출





###### (5) 데이터 분석 과제 추진 시 고려해야 하는 우선순위 평가 기준

분석 과제 우선순위 선정 매트릭스

|    Difficult    |  1   |  2   |
| :-------------: | :--: | :--: |
|      Easy       |  3   |  4   |
| 난이도 / 시급성 | 현재 | 미래 |

1. 
   - 전략적 중요도가 높아 경영에 미치는 영향이 크므로 현재 시급하게 추진이 필요함
   - 난이도가 높아 현재 수준에서 과제를 바로 적용하기에 어려움

2. 
   - 현재 시점에서는 전략적 중요도가 높지 않지만 중장기적 관점에서는 반드시 추진되어야 함
   - 분석과제를 바로 적용하기에는 난이도가 높음

3.  (가장 우선적인 과제 적용이 필요한 분면이다 (3사분면))
   - 전략적 중요도가 높아 현재 시점에 전략적 가치를 두고 있음
   - 과제 추진의 난이도가 어렵지 않아 우선적으로 바로 적용 가능할 필요성이 있음

4. 
   - 전략적 중요도가 높지 않아 중장기적 관점에 과제 추진이 바람직함
   - 과제를 바로 적용하는 것은 어렵지 않음

분석 과제의 적용 우선순위를 시급성에 둔다면 3 > 4 > 2 영역 순이며, 우선순위 기준을 난이도에 둔다면 3 > 1 > 2 영역 순으로 의사결정을 할 수 있다.



##### 3) 데이터 분석 방안

###### (1) 빅데이터 분석 방법론 개념

- 데이터 분석 방법론의 구성요소에는 절차, 방법, 도구와 기법, 템플릿과 산출물이 있다.



###### (2) 빅데이터 분석 방법론 계층

- 단계(Phase)
  - 프로세스 그룹을 통하여 완성된 단계별 산출물이 생성, 기준선으로 설정 관리하며, 버전 관리 등을 통한 통제
- 태스크(Task)
  - 단계를 구성하는 단위 활동, 물리적 또는 논리적 단위로 품질 검토의 항목이 될 수 있음
- 스텝(Step)
  - 입력자료(Input), 처리 및 도구(Process & Tool), 출력자료(Output)로 구성된 단위 프로세스(Unit Process)

> 기준선(Baseline) : 소프트웨어 개발의 특정 시점에서 형상 항목이 소프트웨어 개발에 하나의 완전한 산출물로써 쓰여질 수 있는 상태의 집합이다.
>
> 버전 관리(Configuration Management) : 동일한 소스 코드에 대한 여러버전을 관리하는 기법이다.



###### (3) 빅데이터 분석 방법론의 분석 절차

1. 분석 기획
   1. 비즈니스 이해 및 범위 설정
   2. 프로젝트 정의 및 계획 수립
   3. 프로젝트 위험계획 수립
2. 데이터 준비(Preparing)
   1. 필요 데이터 정의
   2. 데이터 스토어 설계
   3. 데이터 수집 및 정합성 검증
3. 데이터 분석
   1. 분석용 데이터 준비
   2. 텍스트 분석
   3. 탐색적 분석(EDA)
   4. 모델링
   5. 모델 평가 및 검증
   6. 모델 적용 및 운영 방안수립
4. 시스템 구현
   1. 설계 및 구현
   2. 시스템 테스트 및 운영
5. 평가 및 전개
   1. 모델 발전 계획 수립
   2. 프로젝트 평가 보고





###### (4) 분석 방법론 유형

1. KDD 분석 방법론
   1. KDD(Knowledge Discovery in Database) 분석 방법론 개념 : 1996년 Fayyad가 프로파일링 기술을 기반으로 통계적 패턴이나 지식을 찾기 위해 체계적으로 정리한 방법론이다.

   2. KDD 분석 방법론의 절차
      1. 데이터 세트 선택(Selection)
         - 분석 대상의 비즈니스 도메인에 대한 이해와 프로젝트의 목표 설정
         - 데이터베이스 또는 원시 데이터에서 선택 혹은 추가적으로 생성
         
      2. 데이터 전처리(Preprocessing)
         - 노이즈, 이상값, 결측값 등을 제거
         - 추가로 요구되는 데이터 세트가 있을 경우 데이터 세트 선택, 프로세스 재실행
         
      3. 데이터 변환(Transformation)

         - 변수를 찾고, 데이터 차원 축소
         - 데이터 마이닝이 효율적으로 적용될 수 있도록 데이터 세트로 변경

      4. 데이터 마이닝(Data Mining)

         - 분석 목적에 맞는 데이터 마이닝 기법, 알고리즘 선택 패턴 찾기 데이터 분류, 예측작업
         - 필요에 따라 데이터 전처리, 변환 프로세스 변환 가능

      5. 데이터 마이닝 결과 평가(Interpretation/Evaluation)

         - 분석 결과에 대한 해석/평가, 발견된 지식 활용
         - 필요시 선택부터 마이닝까지 프로세스 반복 수행

         > 차원 축소(Dimensionality Reduction) : 목적에 따라 데이터의 양을 줄이는 기법이다.
         >
         > 데이터 마이닝 : 대규모로 저장된 데이터 안에서 체계적이고 자동적으로 통계적 규칙이나 패턴을 찾아내는 기법이다.

   3. CRISP-DM 분석 방법론

      1. CRISP-DM(Cross Industry Standard Process for Data Mining) 분석 방법론의 개념

         - 비즈니스의 이해를 바탕으로 데이터 분석 목적의 6단계로 진행되는 데이터 마이닝 방법론
         - 1996년 유런연합의 ESPRIT 프로젝트에서 시작한 방법론으로 1997년 SPSS 등이 참여하였으나 현재에는 중단되었다.

      2. CRISP-DM 방법론의 구성

         | 구성                             | 설명                                                         |
         | -------------------------------- | ------------------------------------------------------------ |
         | 단계(Phase)                      | 최상위 레벨                                                  |
         | 일반화 태스크(Generic Tasks)     | 데이터 마이닝의 단일 프로세스를 완전하게 수행하는 단위<br>각 단계는 일반화 태스크 포함 |
         | 세분화 태스트(Specialized Tasks) | 일반화 태스크를 구체적으로 수행하는 레벨<br>예) 데이터 정제의 일반화 태스크는 범주형, 연속형 데이터 정제 등으로 구체화된 세분화 태스크 |
         | 프로세스 실행(Process Instances) | 데이터 마이닝을 위한 구체적인 실행                           |

      3. CRISP-DM 분석 방법론의 분석 절차 

         - 업무 이해 > 데이터 이해 > 데이터 준비 > 모델링 > 평가 > 전개

   4. SEMMA 분석 방법론

      1. SEMMA (Sampling Exploration Modification Modeling Assessment) 분석 방법론 개념

         - 분석 솔루션 업체 SAS사가 주도한 통계 중심의 5단계(샘플링 > 탐색 > 수정 > 모델링 > 검증) 방법론이다.

      2. SEMMA 분석 방법론의 분석 절차

         1. 샘플링 (Sampling)
            - 통계적 추출, 조건 추출을 통한 분석 데이터 생성
            - 비용 절감 및 모델 평가를 위한 데이터 준비
         2. 탐색(Exploration)
            - 기초 통계, 그래프 탐색, 요인별 분할표, 클러스터링, 변수 유의성 및 상관 분석을 통한 분석 데이터 탐색
            - 데이터 조감을 통한 데이터 오류 검색
            - 모델의 효율 증대
            - 데이터 현황을 통해 비즈니스 이해, 아이디어를 위해 이상현상, 변화 등을 탐색
         3. 수정(Modification)
            - 수량화, 표준화, 각종 변화, 그룹화를 통한 분석 데이터 수정/변환
            - 데이터가 지닌 정보의 표현 극대화
            - 최적의 모델을 구축할 수 있도록 다양한 형태로 변수를 생성, 선택, 변형
         4. 모델링(Modeling)
            - 신경망, 의사결정나무, 로지스틱 회귀 분석, 전통적 통계를 이용한 모델 구축
            - 데이터의 숨겨진 패턴 발견
            - 하나의 비즈니스 문제 해결을 위해 특수한 모델과 알고리즘 적용 가능
         5. 검증(Assessment)
            - 모델 평가 검증
            - 서로 다른 모델을 동시에 비교
            - 추가 부석 수행 여부 결정

         > 신경망 (Neural Network) : 컴퓨터에서 사람의 두뇌와 비슷한 방식으로 정보를 처리하기 위한 알고리즘이다.
         >
         > 의사결정나무(Decision Tree) : 데이터들이 가진 속성들로부터 분할 기준 속성을 판별하고, 분할 기준 속성을 판별하고, 분할 기준 속성에 따라 트리 형태로 모델링하는 분류 예측 모델이다.
         >
         > 로지스틱 회귀 분석(Logistic Regression) : 종속변수가 범주형이면서 0 또는 1인 경우 사용하는 회귀 분석이다.






#### 2 분석 작업 계획

##### 1) 데이터 확보 계획

###### (1) 데이터 획득 방안 수립

- 내외부의 다양한 시스템으로부터 정형/비정형/반정형 데이터를 수집하기 위한 구체적인 방안을 수립한다.
- 내부 데이터 획득에는 부서 간 업무협조와 개인정보보호 및 정보보안과 관련된 문제점을 사전에 점검하고, 외부 데이터 획득은 시스템 간 다양한 인터페이스 및 법적인 문제점을 고려하여 상세한 데이터 획득 계획을 수립한다.



###### (2) 데이터 확보 계획 수립 절차

1. 목표 정의
   - 성과 목표 정의
   - 성과 지표 설정
     - 비즈니스 도메인 특성 적용
     - 구체적인 성과목표 정의
     - 성과측정을 위한 지표 도출
2. 요구사항 도출
   - 데이터 및 기술 지원 등과 관련된 요구사항 도출
     - 필요 데이터 확보 및 관리 계획
     - 데이터 정제 수준, 데이터 저장 형태
     - 기존 시스템 및 도구 활용 여부
     - 플랫폼 구축 여부
3. 예산안 수립
   - 자원 및 예산 수립
     - 데이터 확보, 구축, 정비, 관리 예산
4. 계획 수립
   - 인력 투입 방안
   - 일정 관리
   - 위험 및 품질관리
     - 프로젝트 관리 계획 수립
     - 범위, 일정, 인력, 의사소통 방안 수립



##### 2) 분석 절차 및 작업 계획

###### (1) 빅데이터 분석 절차

문제 인식 > 연구조사 > 모형화 > 자료 수집 > 자료 분석 > 분석결과 공유

###### (2) 빅데이터 분석 작업 WBS 설정

데이터 분석 과제 정의 > 데이터 준비 및 탐색 > 데이터 분석 모델링 및 검증 > 산출물 정리






### CHAPTER03 데이터 수집 및 저장 계획

#### 1 데이터 수집 및 전환

데이터 처리 기술

- 데이터 필터링
  - 정형데이터는 오류 발견, 보정, 삭제, 중복성 검사 과정을 통해 필터링
  - 비정형 데이터는 자연어 처리, 기계학습과 같은 추가 기술을 적용하여 오류 데이터, 중복 데이터와 같은 저품질 데이터 필터링
  - 활용 목적에 맞지 않는 정보는 필터링하여 분석시간을 단축, 저장 공간을 효율적으로 활용
- 데이터 변환
  - 다양한 형식으로 수집된 데이터를 분석이 쉽도록 일관성 있는 형식으로 변환
  - 데이터 변환에는 평활화, 집계, 일반화, 정규화, 속성 생성 기술을 사용
- 데이터 정제
  - 수집된 데이터의 불일치성을 교정하기 위한 방식으로 결측값 처리, 잡음(Noise) 처리 기술 활용
- 데이터 통합
  - 출처가 다른 상호 연관성이 있는 데이터들을 하나로 결합하는 기술
  - 데이터 통합 시 같은 데이터가 입력될 수 있으므로 연관 관계 분석 등을 통해 중복 데이터 검출 필요
  - 데이터 통합 전.후 수치.통계 등 데이터값들이 일치할 수 있도록 검증
- 데이터 축소
  - 분석에 필요한 데이터를 축소하여 고유한 특성은 손상되지 않도록 분석에 대한 효율성 증대

> 결측값(Missing Value) : 입력이 누락된 값을 의미



##### 1) 데이터 수집

###### (1) 데이터 수집 프로세스

1. 수집 데이터 도출
   - 수집 데이터 도출은 빅데이터 서비스 제공 시 서비스 품질을 결정하는 중요한 핵심 업무
   - 데이터 도메인의 분석 노하우가 있는 내.외부 전문가 의견을 수렴하여 분석 목적에 맞는 데이터 도출 필요
2. 목록 작성
   - 수집 가능성 여부, 보안 문제, 세부 데이터 항목(품질) 및 비용 등을 검토하여 데이터 수집 목록 작성
3. 데이터 소유기관 파악 및 협의
   - 데이터 소유자의 데이터 개발 현황/조건, 적용기술, 보안 사항 등을 파악하고 필요한 협의 진행
   - 데이터 수집 관련 보안 사항, 개인정보보호 관련 문제 등의 점검 필수
4. 데이터 유형 분류 및 확인
   - 수집 대상 데이터 유형을 분류하고 데이터 포맷 등 확인
5. 수집 기술 선정
   - 데이터 유형 및 포맷 등에 맞는 수집 기술 선정
   - 수집 기술은 데이터 소스로부터 다양한 유형의 데이터를 수집하기 위해 확장성, 안정성, 실시간성 및 유연성 확보 필요
6. 수집 계획서 작성
   - 수집 대상 '데이터 출처, 수집 기술, 수집 주기 및 수집 담당자의 주요 업무' 등 을 반영하여 계획서 작성
7. 수집 주기 결정
   - 데이터 유형에 따라 배치 또는 실시간 방식 적용
8. 데이터 수집 실행
   - 사전 테스트 진행하고 데이터 수집 시행



###### (2) 수집 데이터의 대상

데이터의 위치에 따라 내부 데이터와 외부 데이터로 구분됨

- 내부 데이터
  - 조직(인프라) 내부에 데이터가 위치하여, 데이터 담당자와 수집 주기 및 방법 등을 협의하여 데이터를 수집
  - 내부 조직 간 협의를 통한 데이터 수집
  - 주로 수집이 용이한 정형 데이터
  - 서비스의 수명 주기 관리가 용이
- 외부 데이터
  - 조직(인프라) 외부에 데이터가 위치하며, 특정 기관의 담당자 협의 또는 데이터 전문 업체를 통해 데이터를 수집
  - 공공 데이터의 경우에는 공공 데이터 포털을 통해 Open API 또는 파일을 통해 수집
  - 외부 조직과 협의, 데이터 구매, 웹상의 오픈 데이터를 통한 데이터 수집
  - 주로 수집이 어려운 비정형 데이터



원천 데이터 예시

| 구분        | 분야     | 예시                                                         |
| ----------- | -------- | ------------------------------------------------------------ |
| 내부 데이터 | 서비스   | SCM, ERP, CRM, 포털, 원장정보 시스템 인증, 거래 시스템 등    |
| 내부 데이터 | 네트워크 | 백본, 방화벽, 스위치, IPS, IDS                               |
| 내부 데이터 | 마케팅   | VOC 접수 데이터, 고객 포털 시스템 등                         |
| 외부 데이터 | 소셜     | SNS, 커뮤니티, 게시판                                        |
| 외부 데이터 | 네트워크 | 센서 데이터, 장비 간 발생 로그(M2M)                          |
| 외부 데이터 | 공공     | 정부 공개 경제, 의료, 지역정보, 공공 정책, 과학, 교육, 기술 등의 공공데이터(LOD) |

> SCM(Supply Chain Management) : 부품 제공업자로부터 생산자, 배포자, 고객에 이르는 물류의 흐름을 하나의 가치사슬 관점에서 파악하고 필요한 정보가 원한히 흐르도록 지원하는 시스템이다.
>
> ERP(Enterprise Resource Planning) : 전사전 자원 관리는 회사의 모든 정보뿐만 아니라, 공급사슬관리, 고객의 주문정보까지 포함하여 통합적으로 관리하는 시스템이다.
>
> CRM(Customer Relationship Management) : 고객 관계관리라는 소비자들을 자신의 고객으로 만들고, 이를 장기간 유지하고자 하는 경영방식으로 내부 정보를 분석하고 저장하는 데 사용하는 광대한 분야를 아우르는 방법
>
> IPS (Intrusion Prevention System) : 인터넷 웜 등의 악성코드 및 해킹 등으로 인한 유해 트래픽을 차단해주는 솔루션으로 내부 네트워크로 침입하는 네트워크 패킷을 찾아 제어하는 기능을 가진다.
>
> IDS(Intrusion Detection System) : 해커들과 비 인가된 사용자가 시스템을 조작하는 것을 탐지하는 솔루션으로 방화벽이 탐지할 수 없는 여러 종류의 악의적 네트워크 트래픽을 탐지하여 로그를 남긴다.
>
> M2M(Machine To Machine) : 모든 사물에 센서 통신 기능을 부과하여 지능적으로 정보를 수집하고, 상호 전달하는 기술을 의미한다.
>
> LOD(Linked Open Data) : 웹상에 존재하는 데이터를 개별 URI (Uniform Resource Identifier)로 식별하고, 각 URI에 링크 정보를 부여함으로써 상호 연결된 웹을 지향하는 오픈 데이터이다.



###### (3) 데이터 수집 방식 및 기술

1. 정형 데이터 수집 방식 및 기술

   - ETL, FTP, API, DB2DB, RSync, 스쿱(Sqoop)

     > 스쿱 : 커넥터(Connector)를 사용하여 관계형 데이터베이스와 하둡 간 데이터 전송 기능을 제공하는 기술. 스쿱은 모든 적재 과정을 자동화하고 병렬 처리 방식으로 작업
     >
     > RSync : 서버.클라이언트 방식으로 수집 대상 시스템과 1:1로 파일과 디렉토리를 동기화 하는 응용 프로그램 활용 기술

   - FTP 유형

     - Active FTP
       - 클라이언트가 데이터를 수신받을 포트를 서버에 알려주면, 서버가 자신의 20번 포트를 통해 클라이언트의 임의의 포트로 데이터를 전송해 주는 방식
       - 명령은 21번 포트, 데이터는 20번 포트를 사용

     - Passive FTP
       - 서버가 데이터를 송신해줄 임의의 포트를 클라이언트에 알려주면 클라이언트가 서버의 임의의 포트로 접속해서 데이터를 가져가는 방식
       - 명령은 21번 포트, 데이터는 1024 이후의 포트를 가져가는 방식

   - 스쿱 특징

     - 벌크 임포트 지원
     - 데이터 전송 병렬화
     - 직접 입력 제공: RDB에 매핑해서 HBase와 Hive에 직접 import 제공
     - 프로그래밍 방식의 데이터 인터랙션 : 자바 클래스 생성을 통한 데이터 상호작용

2. 비정형 데이터 수집 방식 및 기술
   - 크롤링(Crawling), RSS(Rich Site Summary), Open API, 스크래파이(Scrapy), 아파치 카프카(Apache Kafka)
     - RSS : 블로그, 뉴스 등의 새로운 글을 XML 기반으로 정보를 배포하는 프로토콜을 활용하여 데이터를 수집하는 기술
     - Open API : 응용 프로그램을 통해 실시간으로 데이터를 수신할 수 있도록 공개된 API를 이용하여 데이터를 수집하는 기술
     - 스크래파이 : 웹 사이트를 크롤링하여 구조화된 데이터를 수집하는 파이썬(Python) 기반의 애플리케이션 프레임워크로서 데이터 마이닝, 정보 처리, 이력 기록 같은 다양한 애플리케이션에 사용되는 수집 기술
     - 아파치 카프카 : 대용량 실시간 로그 처리를 위해 기존 메시징 시스템과 유사하게 레코드 스트림을 발행(Publish), 구독(Subscriber)하는 방식의 분산 스트리밍 플랫폼 기술

3. 반정형 데이터 수집 방식 및 기술

   - 센싱(Sensing) : 센서로부터 수집 및 생성된 데이터를 네트워크를 통해 수집 및 활용
   - 스트리밍(Streaming) : 네트워크를 통해 센서 데이터 및 오디오, 비디오 등의 미디어 데이터를 실시간으로 수집하는 기술
   - 플럼(Flume) : 스트리밍 데이터 흐름(Data Flow)을 비동기 방식으로 처리하는 분산형 로그 수집 기술
   - 스크라이브(Scribe) : 다수의 서버로부터 실시간으로 스트리밍되는 로그 데이터를 수집하여 분산 시스템에 데이터를 저장하는 대용량 실시간 로그 수집 기술
   - 척와(Chukwa) : 대규모 분산 시스템 모니터링을 위해 에이전트(Agent)와 컬렉터(Collector) 구성을 통해 데이터를 수집하고, 수집된 데이터를 하둡 파일 시스템(HDFS)에 저장하는 기능을 제공하는 데이터 수집 기술



##### 2) 데이터 유형 및 속성 파악

###### (1) 데이터 유형

- 구조관점 
  - 정형 데이터 : RDB, 스프레드시트
  - 반정형 데이터 : XML, HTML, 웹로그, JSON, RSS, 센서 데이터
  - 비정형 데이터 : SNS, 웹 게시판, 텍스트, 이미지, 오디오, 비디오
- 시간관점
  - 실시간 데이터 : 센서 데이터, 시스템 로그, 네트워크 장비 로그, 보안 장비 로그, 알람
  - 비실시간 데이터 : 통계, 웹 로그, 구매 정보, 서비스 로그, 디지털 헬스케어 정보
- 저장 형태
  - 파일 데이터 : 로그, 텍스트, 스프레드시트
  - 데이터베이스 데이터 : RDBMS, NoSQL, 인메모리 데이터베이스
  - 콘텐츠 데이터 : 텍스트, 이미지, 오디오, 비디오 등과 같이 개별적으로 데이터 객체로 구분될 수 있는 미디어 데이터
  - 스트림 데이터 : 센서 데이터, HTTP 트랜잭션, 알람 등과 같이 네트워크를 통해서 실시간으로 전송되는 데이터



###### (2) 데이터 속성 파악

데이터 형태에 따른 분류

| 구분                                 | 형태                | 사례                  | 특징                                  |
| ------------------------------------ | ------------------- | --------------------- | ------------------------------------- |
| 정성적 데이터<br>(Qualitative data)  | 언어, 문자 등       | 기업 매출이 증가함 등 | 저장.검색.분석에 많은 비용 소모       |
| 정량적 데이터<br>(Quantitative data) | 수치, 도형, 기호 등 | 키, 생일, 주가 등     | 정형화가 된 데이터로 비용 소모가 적음 |



데이터 속성

- 범주형 

  - 명목형(Nominal)
    - 명사형으로 변수나 변수의 크기와 상관없고, 의미가 없이 이름만 의미를 부여할 수 있는 경우
    - 예) 스마트폰 브랜드(삼성=1, LG=2, 애플=3), 1+2 = 3은 의미 없음
  - 순서형(Ordinal)
    - 변수가 어떤 기준에 따라 순서에 의미를 부여할 수 있는 경우
    - 예) 병원수준(의원=1, 종합병원=2, 대학병원=3), 화장실 상태(양호=3, 보통=2, 나쁨=1)

- 수치형

  - 이산형(Discrete)

    - 변수가 취할 수 있는 값을 하나하나 셀 수 있는 경우
    - 예) 문 개수, 시험문제 중 틀린 개수, 자동차 사기전까지 대리점 방문 횟수

  - 연속형(Continous)

    - 변수가 구간 안의 모든 값을 가질 수 있는 경우
    - 예) 노인들의 키, 양의 정수 구간 안의 모든 값

    > 나이는 시간이 지남에 따라 계속 늘어나는 연속형 변수이지만, 1년 단위로 측정되면 이산형 변수이다.



데이터 측정 척도

- 명목 척도(Nominal Scale)
  - 관측 대상을 임의의 범주로 분류한 후 기호나 숫자를 부여하는 방법
  - 분류의 수치화이고 ,척도 값이 분류의 의미만을 가짐
  - 대표적으로 출신 국가 분류, 고객 구분, 직업 구분, 주택보유 여부 등을 나타낼 때 명목 척도 활용
  - 예) 예비역 구분(현역, 예비역), 혈액형(A, B, O AB), 지역 번호 등
- 서열 척도 / 순위 척도(Ordinal Scale)
  - 비계량적인 변수를 관측하기 위한 관측 방법
  - 여러 관측 대상을 임의의 기준에 따라 상대적인 비교 및 순위화를 통해 관측하는 방법
  - 서열의 순서화로 척도 값이 분류 및 서열 순서를 가짐
  - 예) 맛집 별점, 음료수의 선호도 조사(1, 2, 3위) => 순서만 의미가 있고, 수치의 크기나 차이는 의미가 없음
- 등간 척도 / 간격 척도 / 거리 척도 (Interval Scale)
  - 비계량적인 변수를 정량적인 방법으로 측정하기 위하여 사용
  - 비계량적 변수의 경우 수치적으로 평가하기 어려우므로 상, 중, 하 등으로 평가
  - 예) 미세먼지 수치, 당뇨 수치<br> - 3점 척도 : (선호함, 보통, 싫어함), (상,중,하)<br> - 5점 척도 : (매우선호, 선호, 보통, 싫어함, 매우 싫어함), (상, 중상, 중, 중하, 하)
- 비율 척도(Ratio Scale)
  - 균등 간격에 절대 영점이 있고, 비율 계산이 가능한 척도
  - 가장 전형적인 양적 변수로 쓰임
  - 순서뿐만 아니라 그 간격도 의미가 있음
  - 예) 나이, 키, 금액, 거리, 넓이, 소득 등 => 이 경우 금액의 비율, 무게의 비율이 의미가 있으며, 평균 금액, 평균 거리 등도 의미가 있음





