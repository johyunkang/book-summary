# 빅데이터분석기사 2021

## 2 빅데이터 탐색

### CHAPTER01 데이터 전처리

#### 1 데이터 정제

##### 1) 데이터 정제

###### (1) 데이터 전처리의 중요성

- 반드시 거쳐야 하는 과정
- 전처리는 반복적으로 수행
- 가장 많은 시간이 소요되는 단계가 데이터 수집과 전처리 단계
- 데이터 정제 > 결측값 처리 > 이상값 처리 > 분석 변수 처리 순으로 진행



###### (2) 데이터 정제(Data Cleansing) 개념

결측값을 채우거나 이상값을 제거하는 과정을 통해 데이터의 신뢰도를 높이는 작업



###### (3) 데이터 정체 절차

1. 오류원인 분석 : 원천 데이터 오류 또는 빅데이터 플로우 문제로 발생 (결측값, 노이즈, 이상값)
2. 데이터 정제 대상 선정 : 모든 데이터가 대상
3. 데이터 정제 방법 결정 : 삭제, 대체, 예측값 삽입



###### (4) 데이터 정제 기술

데이터 일관성을 위한 정제 기법 : 변환(Transform), 파싱(Parsing), 보강(Enhancement)

데이터 정제 기술

- ETL, 맵리듀스(Map Reduce), 스파크/스톰(Spark/Storm), CEP(Complex Event Processing), 피그(Pig), 플럼(Flume)



###### (5) 데이터 세분화

개념

- 데이터를 기준에 따라 나누고, 선택한 매개변수를 기반으로 유사한 데이터를 그룹화하여 효율적으로 사용할 수 있는 프로세스이다.

데이터 세분화 방법

| 방법           | 설명                                                         | 기법                             |
| -------------- | ------------------------------------------------------------ | -------------------------------- |
| 계층적 방법    | 사전에 군집수를 정하지 않고 단계적으로 단계별 군집결과를 산출 | 응집분석법<br>분할분석법         |
| 비 계층적 방법 | 군집을 위한 소집단의 개수를 정해놓고 각 객체 중 하나의 소집단으로 배정 | 인공신경망 모델<br>K-평균 군집화 |



##### 2) 데이터 결측값 처리

###### (1) 데이터 결측값(Data Missing Value) 개념

- 결측값은 입력이 누락된 값
- NA, 999999, NULL 등으로 표현



###### (2) 데이터 결측값 종류

- 완전 무작위 결측(MCAR; Missing Completely At Random)
- 무작위 결측(MAR; Missing At Random)
- 비 무작위 결측(MNAR; Missing Not At Random)

###### (3) 데이터 결측값 처리 절차

1. 결측값 식별 : 현황 파악
2. 결측값 부호화
   - 컴퓨터가 처리 가능한 형태로 부호화
   - NA(Not Available) : 기록되지 않은값
   - NaN(Not a Number) : 수학적으로 정의되지 않은 값
   - inf(infinite) : 무한대
   - NULL : 값이 없음
3. 결측값 대체 : 결측값을 자료형에 맞춰 대체 알고리즘을 통해 결측값을 처리


###### (4) 데이터 결측값 처리 방법

- 단순 대치법

  1. 완전 분석법(Completes Analysis)

  2. 평균 대치법(Mean Imputation)

  3. 단순 확률 대치법(Single Stochastic Imputation)
     1. 핫덱(Hot-Deck) 대체 : 비슷한 성향을 가진 응답자의 자료로 대체. 표본조사에 흔히 사용
     2. 콜드덱(Cold-Deck) 대체 : 핫덱과 비슷하나 대체 자료를 현재 진행 중인 연구에서 얻는것이 아니라 외부 출처 또는 이전의 비슷한 연구에서 가져오는 방법
     3. 혼합 방법 : 몇 가지 다른 방법을 혼합하는 방법

- 다중 대치법

  - 적용방식 : 대치 > 분석 > 결합


##### 3) 데이터 이상값 처리
###### (1)  데이터 이상값(Data Outlier) 개념

- 범위에서 많이 벗어난 아주 작은 값이나 아주 큰 값

###### (2) 데이터 이상값 발생 원인

- 데이터 입력 오류 : 100 입력해야 하는데 1000 입력
- 측정 오류 : 몸무게 측정하는데 9개 체중계 정상, 1개 비정상
- 실험 오류 : 100미터 달리기 하는데, 한 선수가 출발 신호 못 듣고 늦게 출발
- 고의적인 이상값 : 자기 보고식 측정(Self Reported Measures)에서 나타나는 에러. 음주량 조사에서 10대는 보통 음주량을 적게 기입
- 표본추출 에러 : 샘플링을 잘못한 경우. 대학 신입생 키 조사하는데, 농구선수가 포함되어 있는 경우



###### (3) 데이터 이상값 검출 방법

- 개별 데이터 관찰
- 통곗값
- 시각화 : 확률 밀도 함수, 히스토그램, 시계열 차트
- 머신러닝 기법 : K-평균 알고리즘
- 마할라노비스 거리(Mahalanobis Distance) : 관측치가 평균으로 부터 벗어난 정도를 측정하는 통계량 기법
- LOF(Local Outlier Factor) : 관측치 주변 밀도와 근접한 관측치 주변 밀도의 상대적인 비교를 통해 이상값을 탐색하는 기법
- iForest(Isolation Forest) : 의사결정나무(Decisiion Tree)을 이용하여 이상값을 탐지하는 방법

통계기법을 이용한 데이터 이상값 검출 방법

| 검출기법                                  | 설명                                                         |
| ----------------------------------------- | ------------------------------------------------------------ |
| ESD(Extreme Studentized Deviation)        | 평균(&mu;) 으로 부터 3 표준편차(&sigma;) 떨어진 값(각 0.15%)을 이상값으로 판단<br>&mu; - 3&sigma; < data < &mu; + 3&sigma; |
| 기하평균 활용한 방법                      | 기하평균으로부터 2.5표준편차(&sigma;) 떨어진 값을 이상값으로 판단<br> 기하평균 - 2.5 x &sigma; < data < 기하평균 + 2.5 x &sigma; |
| 사분위 수를 이용한 방법                   | 사분위간범위(Q<sub>3</sub> - Q<sub>1</sub>) : IQR 의 1.5배 이상 떨어진 값을 이상값으로 판단 <br> Q<sub>1</sub> - 1.5 x (Q<sub>3</sub> - Q<sub>1</sub>) < data < Q<sub>3</sub> + 1.5 x (Q<sub>3</sub> - Q<sub>1</sub>) |
| 표준화 점수(Z-score)를 활용한 이상값 검출 | 정규분포를 따르는 관측치들이 자료의 중심(평균)에서 얼마나 떨어져 있는지를 나타냄에 따라서 이상값을 검출 |
| 딕슨의 Q 검정(Dixon Q-Test)               | 오름차순 정렬 데이터에서 범위에 대한 관측치 간의 차이의 비율을 활용하여 이상값 여부를 검정하는 방법 <br> 데이터 수가 30개 미만인 경우에 적합 |
| 그럽스 T-검정<br>(Grubbs T-Test)          | 정규분포를 만족하는 단변량 자료에서 이상값을 검정하는 방법   |
| 카이제곱 검정<br>(Chi-Squre Test)         | 데이터가 정규분포를 만족하나, 자료의 수가 적은 경우에 이상값을 검정하는 방법 |

> 단변량 자료(Univariate Data) : 단위에 대해 하나의 속성만 측정하여 얻게 되는 변수에 대한 자료이다.



###### (4) 데이터 이상값 처리

- 이상값을 **반드시 제거**해야 하는 것은 아님. 이상값을 처리할지는 분석의 목적에 따라 적절한 판단이 필요

- 데이터 이상값 처리 기법

  1. 삭제(Deleting Obervations)

     - 기하평균을 이용한 제거

     - 하단, 상단 % 이용한 제거

  2. 대체법(Imputation)
     - ESD를 사용할 경우 상한값은 &mu; + 3&sigma; , 하한값은 &mu; - 3&sigma;
     - 이상값을 평균이나 중앙값으로 대체

  3. 변환(Transformation)
     - 극단적인 이상값 발생 시, 자연로그를 취해서 값을 감소 시키기
     - 상하한값을 벗어나는 값은, 상하한값으로 바꾸어 활용하는 극단값 조정(Winsorizing) 방법도 활용
  4. 박스플롯 해석을 통한 이상값 제거
     - 사분위수를 이용하여 제거, 수염(Whiskers) 밖에 있는 값을 이상값으로 판단
  5. 분류하여 처리
     - 이상값이 많을 경우, 그 이상값을 하나의 그룹으로 묶어 서로 다른 그룹으로 통계적인 분석을 실행하여 처리



#### 2 분석 변수 처리

##### 1) 변수 선택

###### (1) 변수(Feature) 개념

- 예측을 수행하는데 사용되는 입력변수이다.
- RDBMS에서 '속성(열)' 이라고 부르는 것을 머신러닝에서는 통계학의 영향으로 '변수(Feature)'라고 한다.

변수 명칭

| 유형      | 명칭                                                         |
| --------- | ------------------------------------------------------------ |
| 알려진 값 | **변수(Feature)**, 속성(Attribute), 예측변수(Predictor), **차원(Dimension)**, 관측치(Observation), **독립변수(Independent Variable)** |
| 예측 값   | **라벨(Label)**, 클래스(Class), 목푯값(Target), 반응(Response), **종속변수(Dependent Variable)** |



###### (2)  변수 유형

1. 인과관계
   1. 독립변수
      - 다른 변수에 영향을 받지 않고 종속변수에 영향을 주는 변수
   2. 종속변수
      - 다른 변수로부터 영향을 받는 변수
2. 변수속성
   1. 범주형(Categorical) : 범위와 순서가 있는 변수
      1. 명목형(Nominal)
         - 의미가 없이 이름만 의미를 부여할 수 있는 경우
         - 예) 삼성=1, 엘지=2, 애플=3
      2. 순서형(Ordinal)
         - 순서에 의미를 부여할 수 있는 경우
         - 예) 병원수준(의원=1, 종합병원=2, 대학병원=3), 화장실상태(양호=3, 보통=2, 나쁨=1)
   2. 수치형(Measure) : 수치로 표현되는 변수
      1. 이산형(Discrete)
         - 값을 하나하나 셀 수 있는 경우
         - 예) 틀린 개수, 책의 개수
      2. 연속형(Continuous)
         - 구간 안의 모든 값을 가질 수 있는 경우
         - 예) 몸무게, 키



###### (3) 변수 선택

- 개념 : 독립변수 중 종속변수에 가장 관련성이 높은 변수만을 선정하는 기법
- 특징 : 사용자가 해석하기 쉽게 모델을 단순화해주고, 훈련 시간 축소, 차원의 저주 방지, 과적합(Over-fitting)을 줄여줌
- 변수 선택 기법
  - 필터 기법(Filter Method)
    - 계산 속도가 빠르고 변수 간 상관관계를 알아내는 데 적합
  - 래퍼 기법(Wrapper Method)
    - 가장 좋은 성능을 보이는 하위 집합을 선택하는 기법
    - 하위 집합을 반복해서 선택하여 테스트하므로, 그리디 알고리즘에 적합
    - 시간이 오래 걸리고, 과적합의 위험
    - 필터 방법보다 정확도가 높다

래퍼기법 상세

1. RFE (Recursive Feature Elimination) : SVM(Support Vector Machine)을 사용하여 재귀적으로 제거하는 방법
2. SFS (Sequential Feature Selection) : 그리디 알고리즘으로 특성 변수를 하나씩 추가하는 방법
3. 유전 알고리즘(Genetic Algorithm) : 자연세계의 진화과정에 기초한 계산 모델, 최적화 문제를 해결하는 기법
4. 단변량 선택(Univariate Selection) : 하나의 변수선택법으로, 피처와 반응변수 간 관계의 강도를 결정하는 방법. 데이터에 대한 이해를 높일 때 사용
5. mRMR (Minimum Redundancy Maximum Relevance) : 특성 변수의 중복성을 최소화하는 방법으로 종속변수를 잘 예측하면서, 독립변수들과도 중복성이 적은 변수들을 선택하는 방법



임베디드 기법

- 모델의 정확도에 기여하는 변수를 학습한다.
- 적은 계수를 가지는 회귀식을 찾는 방향으로 제약조건을 주어 이를 제거한다.

임베디드 기법 사례

1. 라쏘(LASSO; Least Absolute Shrinkage and Selection Operator) : 가중치의 절댓값의 합을 최소화하는 것을 추가적인 제약조건으로 하는 방법. L1-norm을 통해 제약을 주는 방법
2. 릿지(Ridge) : 가중치들의 제곱합을 최소화하는 것을 추가적인 제약조건으로 하는 방법. L2-norm을 통해 제약을 주는 방법
3. 엘라스틱 넷(Elastic Net) : 라쏘 와 릿지 두 개를 선형 결합한 방법
4. SelectFromModel : 의사결정나무 기반 알고리즘에서 변수를 선택하는 방법


##### 2) 차원축소

###### (1) 차원축소(Dimensionality Reduction) 개념

- 여러 변수의 정보를 최대한 유지하면서 데이터 세트 변수의 개수를 줄이는 탐색적 분석기법
- 목표변수(y)는 사용하지 않고 특성 변수(설명변수)만 사용하기 때문에 비지도 학습 머신러닝 기법

###### (2) 차원축소 특징

- 전체 데이터의 변수들의 정보를 최대한 유지
- 해당 결합변수만으로도 전체변수를 적절히 설명할 수 있어야 한다.
- 다른 분석과정을 위한 전 단계, 분석 수행 후 개선방법, 또는 효과적인 시각화 등의 목적으로 사용
- 저차원으로 학습할 경우, 회귀나 분류, 클러스터링 등의 머신러닝 알고리즘이 더 잘 작동한다.
- 새로운 저차원 변수(Feature) 공간에서 가시적으로 시각화하기도 쉽다.

###### (3) 차원축소 기법

1. 주성분 분석(PCA: Principal Component Analysis)
   - 변수들의 공분산 행렬이나 상관행렬을 이용
   - 선형 연관성이 없는 저차원 공간으로 변환하는 기법
   - 행의 수와 열의 수가 같은 정방행렬에서만 사용
2. 특이값 분해(SVD: Singular Value Decomposition)
   - M x N 차원의 행렬데이터에서 특이값을 추출하고, 이를 통해 주어진 데이터 세트를 효과적으로 축약할 수 있는 기법
3. 요인분석(Factor Analysis)
   - 데이터 안에 관찰할 수 없는 잠재적인 변수(Latent Variable)가 존재한다고 가정
   - 관찰 가능한 데이터를 이용하여 해당 잠재 요인을 도출하고 데이터 안의 구조를 해석하는 기법
   - 사회과학이나 설문 조사 등에서 활용
4. 독립성분분석(ICA: Independent Component Analysis)
   - 다변량의 신호를 통계적으로 독립적인 하부성분으로 분리하여 차원을 축소하는 기법
   - 비정규 분포를 따르게 되는 차원축소 기법
5. 다차원 척도법(MDS: Multi-Dimensional Scaling)
   - 유사성, 비유사성을 측정하여 2차원 또는 3차원 공간상에 점으로 표현하여 개체들 사이의 집단화를 시각적으로 표현하는 분석 방법

###### (4) 차원축소 기법 주요 활용 분야

- 탐색적 데이터 분석부터 정보 결과의 시각화까지 다양하게 활용되고 있다
- 좀 더 쉽게 데이터를 학습하고 모델을 생성하고자 할 때 주로 활용된다.
- 패턴인식이나 추천시스템 구현 결과의 성능 등을 개선할 때도 사용
- 텍스트 데이터에서 주제나 개념 추출
- 이미지 및 사운드 등의 비정형 데이터에서 특징 패턴 추출
- 판매데이터에서 상품 추천시스템 알고리즘 구현
- 다차원 공간의 정보를 저차원으로 시각화
- 공통 요인(Factor)을 추출하여 잠재된 데이터 규칙 발견


##### 4) 파생 변수(Derived Variance) 생성

파생 변수 생성 방법

- 단위 변환 : 하루 24시간을 12시간으로 변환
- 표현형식 변환 : 남/여 데이터를 0/1 이진수로 변환
- 요약 통계량 변환 : 고객별 누적 방문 횟수 집계
- 변수 결합 : 매출액과 방문 횟수 데이터로 1회 평균 매출액 추출

##### 5) 변수 변환(Variable Transformation)

변수 변환 방법

1. 단순 기능 변환(Simple Function)
   - 한 쪽으로 치우친 변수를 변환하여 분석 모형을 적합하게 하는 방법
   - 예) 로그변환($\log$$x$), 역수 변환(1/x), 루트변환($\sqrt x$), 제곱 변환($x$^2^)

2. 비닝(Binning)
   - 기존 데이터를 범주화하기 위해 사용
   - 두 개 이상의 변수의 값에 따라 공변량 비닝(co-variate binning) 수행
   - 예) 수입을 상,중,하의 범주로 나누기
3. 정규화(Normalization)
   - 데이터를 특정 구간으로 바꾸는 척도법
   - 최소-최대 정규화, Z-스코어 정규화 유형이 있음
   - 예) 최소-최대 정규화 $x - x~min~ \over x~max~ - x~min~$
4. 표준화
   - 데이터를 0을 중심으로 양쪽으로 데이터를 분포시키는 방법
   - 표준화와 정규화는 데이터 전처리에서 상호 교환하여 사용
   - 예) Z-스코어 정규화 : Z = $x - X \over s$   : X: 평균, s: 표준편차


